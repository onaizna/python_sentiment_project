{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data (without using a model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(os.path.abspath('..')) \n",
    "from src import config \n",
    "from src.loading_data import load_multiple_excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Using the function to load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load files from base path: c:\\Users\\loren\\OneDrive\\Documenti\\UniMiB\\Data_Processing_and_Analysis\\Lezione\\Lorenzo_project\\src\\../data/raw/\n",
      "Loading: c:\\Users\\loren\\OneDrive\\Documenti\\UniMiB\\Data_Processing_and_Analysis\\Lezione\\Lorenzo_project\\src\\../data/raw/earth_day_tweets_sentiment_50k_(1).xlsx as 'df_ED_sentiment_1'\n",
      "Successfully loaded: earth_day_tweets_sentiment_50k_(1).xlsx\n",
      "Loading: c:\\Users\\loren\\OneDrive\\Documenti\\UniMiB\\Data_Processing_and_Analysis\\Lezione\\Lorenzo_project\\src\\../data/raw/earth_day_tweets_sentiment_50k_(2).xlsx as 'df_ED_sentiment_2'\n",
      "Successfully loaded: earth_day_tweets_sentiment_50k_(2).xlsx\n",
      "Loading: c:\\Users\\loren\\OneDrive\\Documenti\\UniMiB\\Data_Processing_and_Analysis\\Lezione\\Lorenzo_project\\src\\../data/raw/fifa_world_cup_2022_tweets_sentiment_22k.xlsx as 'df_fifa'\n",
      "Successfully loaded: fifa_world_cup_2022_tweets_sentiment_22k.xlsx\n",
      "Loading: c:\\Users\\loren\\OneDrive\\Documenti\\UniMiB\\Data_Processing_and_Analysis\\Lezione\\Lorenzo_project\\src\\../data/raw/generic_27k.xlsx as 'df_generic'\n",
      "Successfully loaded: generic_27k.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define which files and which name files you want to load in the dictionary\n",
    "files_to_load = {\n",
    "    'df_ED_sentiment_1': 'earth_day_tweets_sentiment_50k_(1).xlsx',\n",
    "    'df_ED_sentiment_2': 'earth_day_tweets_sentiment_50k_(2).xlsx',\n",
    "    'df_fifa': 'fifa_world_cup_2022_tweets_sentiment_22k.xlsx',\n",
    "    'df_generic': 'generic_27k.xlsx'\n",
    "}\n",
    "\n",
    "# Call the function to load data\n",
    "dataframes = load_multiple_excel(config.RAW_DATA_PATH, files_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Showing categories distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting specific column renaming...\n",
      "  - Renaming columns in 'df_fifa'...\n",
      "    - Original columns: ['Unnamed: 0', 'Date Created', 'Number of Likes', 'Source of Tweet', 'Tweet', 'Sentiment']\n",
      "    - New columns: ['date_created', 'number_of_likes', 'source_of_tweet', 'tweet', 'sentiment']\n",
      "  - Renaming columns in 'df_generic'...\n",
      "    - Original columns: ['textID', 'text', 'sentiment']\n",
      "    - New columns: ['text_id', 'text', 'sentiment']\n",
      "Column renaming finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Renaming specific columnes ---\n",
    "print(\"Starting specific column renaming...\")\n",
    "\n",
    "# Map for df_fifa: {original_name: new_name}\n",
    "fifa_rename_map = {\n",
    "    'Date Created': 'date_created',\n",
    "    'Number of Likes': 'number_of_likes',\n",
    "    'Source of Tweet': 'source_of_tweet',\n",
    "    'Tweet': 'tweet',\n",
    "    'Sentiment': 'sentiment' \n",
    "}\n",
    "# Map for df_generic: {original_name: new_name}\n",
    "generic_rename_map = {\n",
    "    'textID': 'text_id',\n",
    "}\n",
    "\n",
    "# Applies the renaming at df_fifa if exists\n",
    "if 'df_fifa' in dataframes:\n",
    "    print(\"  - Renaming columns in 'df_fifa'...\")\n",
    "    print(f\"    - Original columns: {dataframes['df_fifa'].columns.tolist()}\")\n",
    "    # removing 'Unnamed: 0' column because it's exacly the index\n",
    "    dataframes['df_fifa'].drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    dataframes['df_fifa'].rename(columns=fifa_rename_map, inplace=True)\n",
    "    print(f\"    - New columns: {dataframes['df_fifa'].columns.tolist()}\")\n",
    "else:\n",
    "    print(\"  - 'df_fifa' not found in dataframes, skipping rename.\")\n",
    "\n",
    "# Applies the renaming at df_generic if exists\n",
    "if 'df_generic' in dataframes:\n",
    "    print(\"  - Renaming columns in 'df_generic'...\")\n",
    "    print(f\"    - Original columns: {dataframes['df_generic'].columns.tolist()}\")\n",
    "    dataframes['df_generic'].rename(columns=generic_rename_map, inplace=True)\n",
    "    print(f\"    - New columns: {dataframes['df_generic'].columns.tolist()}\")\n",
    "else:\n",
    "    print(\"  - 'df_generic' not found in dataframes, skipping rename.\")\n",
    "\n",
    "print(\"Column renaming finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the things that we discovered in the eda notebook, we can look for the categories of each column of each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating DataFrames with unique values specified for DataFrame...\n",
      "\n",
      "Processing: 'df_ED_sentiment_1' -> 'cat_sent_1'\n",
      "  - Columns to analyze: ['sentiment', 'emotion']\n",
      "    - Column 'sentiment': found 2 unique values.\n",
      "    - Column 'emotion': found 4 unique values.\n",
      "  -> Created DataFrame 'cat_sent_1' with shape (4, 2)\n",
      "\n",
      "Processing: 'df_ED_sentiment_2' -> 'cat_sent_2'\n",
      "  - Columns to analyze: ['sentiment', 'emotion']\n",
      "    - Column 'sentiment': found 2 unique values.\n",
      "    - Column 'emotion': found 4 unique values.\n",
      "  -> Created DataFrame 'cat_sent_2' with shape (4, 2)\n",
      "\n",
      "Processing: 'df_fifa' -> 'fifa_sent'\n",
      "  - Columns to analyze: ['sentiment']\n",
      "    - Column 'sentiment': found 3 unique values.\n",
      "  -> Created DataFrame 'fifa_sent' with shape (3, 1)\n",
      "\n",
      "Processing: 'df_generic' -> 'generic_sent'\n",
      "  - Columns to analyze: ['sentiment']\n",
      "    - Column 'sentiment': found 3 unique values.\n",
      "  -> Created DataFrame 'generic_sent' with shape (3, 1)\n",
      "\n",
      "--- Operation complete ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Defining columns that we want to analyze for each original DataFrame\n",
    "#    The keys must match the names used in the 'dataframes' dictionary\n",
    "columns_per_df = {\n",
    "    'df_ED_sentiment_1': ['sentiment', 'emotion'],\n",
    "    'df_ED_sentiment_2': ['sentiment', 'emotion'],\n",
    "    'df_fifa': ['sentiment'],\n",
    "    'df_generic': ['sentiment'] \n",
    "}\n",
    "\n",
    "# 2. Joining the original name_mapping with the new desired names\n",
    "name_mapping = {\n",
    "    'df_ED_sentiment_1': 'cat_sent_1',\n",
    "    'df_ED_sentiment_2': 'cat_sent_2',\n",
    "    'df_fifa': 'fifa_sent',\n",
    "    'df_generic': 'generic_sent' \n",
    "}\n",
    "\n",
    "# 3. Initializing a dictionary to contain the new DataFrames\n",
    "unique_categories_dfs = {}\n",
    "\n",
    "print(\"Start creating DataFrames with unique values specified for DataFrame...\")\n",
    "\n",
    "# 4. Repeats on every original DataFrame that needs to be processed (using name_mapping)\n",
    "for original_name, new_name in name_mapping.items():\n",
    "    \n",
    "    # Controlling if the original DataFrame exists in the 'dataframes' dictionary\n",
    "    if original_name in dataframes:\n",
    "        df_original = dataframes[original_name]\n",
    "        print(f\"\\nProcessing: '{original_name}' -> '{new_name}'\")\n",
    "        \n",
    "        # 5. Recovering the specific column list for this DataFrame\n",
    "        if original_name in columns_per_df:\n",
    "            columns_to_analyze_for_this_df = columns_per_df[original_name]\n",
    "            print(f\"  - Columns to analyze: {columns_to_analyze_for_this_df}\")\n",
    "        else:\n",
    "            # If for some reason we didn't specified the columns for a df that we want to map\n",
    "            print(f\"  - Attention: didn't found specified columns '{original_name}'. Skipped.\")\n",
    "            continue \n",
    "\n",
    "        # Temporary dictionary to contain the unique value Series for this df\n",
    "        unique_series_dict = {}\n",
    "        \n",
    "        # 6. Repeats on the specific columns of this DataFrame\n",
    "        for col_name in columns_to_analyze_for_this_df:\n",
    "            # Checks if the column exists on the current DataFrame\n",
    "            if col_name in df_original.columns:\n",
    "                try:\n",
    "                    # Picks unique values, removes NaN \n",
    "                    # We use .astype(str) before unique() to manage multiple types or objects\n",
    "                    # but for numeric columns like 'number_of_likes', couldn't be ideal\n",
    "                    # If a column is surely numeric, we should treat it in a different way\n",
    "                    # Here we try a generic approach:\n",
    "                    unique_values = df_original[col_name].dropna().unique()\n",
    "                    \n",
    "                    print(f\"    - Column '{col_name}': found {len(unique_values)} unique values.\")\n",
    "                    \n",
    "                    # Creates a Series with these unique values.\n",
    "                    unique_series_dict[col_name] = pd.Series(unique_values, name=col_name)\n",
    "\n",
    "                except Exception as e:\n",
    "                     print(f\"    - Error: couldn't process the column '{col_name}' in '{original_name}': {e}\")\n",
    "            else:\n",
    "                print(f\"    - Column '{col_name}' not found in '{original_name}'. Skipped.\")\n",
    "                \n",
    "        # 7. Creates the new DataFrame concatening the Series as columns\n",
    "        if unique_series_dict: # Goes on only if we found/processed at least a column\n",
    "            # We use pd.concat as before. It will manage different lenghts filling with NaN.\n",
    "            df_new_unique = pd.concat(unique_series_dict.values(), axis=1, keys=unique_series_dict.keys())\n",
    "            \n",
    "            # 8. Adding the new DataFrame at the result dictionary.\n",
    "            unique_categories_dfs[new_name] = df_new_unique\n",
    "            print(f\"  -> Created DataFrame '{new_name}' with shape {df_new_unique.shape}\")\n",
    "        else:\n",
    "            print(f\"  -> Didn't found/processed any valid column for '{original_name}'. didn't create '{new_name}' Dataframe.\")\n",
    "\n",
    "    else:\n",
    "        # If the original name isn't an accepted key for the 'dataframes' dictionary\n",
    "        print(f\"\\nAttention: original DataFrame '{original_name}' not found in 'dataframes' dictionary. Skipped.\")\n",
    "\n",
    "print(\"\\n--- Operation complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categories distribution for each column of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Unique values count for every column of each DataFrame ---\n",
      "\n",
      "--- DataFrame: 'df_ED_sentiment_1' ---\n",
      "Counting unique values for each column:\n",
      "  - Column 'text': 27009 unique values\n",
      "  - Column 'hash_tags': 8566 unique values\n",
      "  - Column 'account_tags': 13643 unique values\n",
      "  - Column 'sentiment': 2 unique values\n",
      "  - Column 'emotion': 4 unique values\n",
      "------------------------------\n",
      "\n",
      "--- DataFrame: 'df_ED_sentiment_2' ---\n",
      "Counting unique values for each column:\n",
      "  - Column 'text': 26185 unique values\n",
      "  - Column 'hash_tags': 8233 unique values\n",
      "  - Column 'account_tags': 13209 unique values\n",
      "  - Column 'sentiment': 2 unique values\n",
      "  - Column 'emotion': 4 unique values\n",
      "------------------------------\n",
      "\n",
      "--- DataFrame: 'df_fifa' ---\n",
      "Counting unique values for each column:\n",
      "  - Column 'date_created': 14412 unique values\n",
      "  - Column 'number_of_likes': 271 unique values\n",
      "  - Column 'source_of_tweet': 109 unique values\n",
      "  - Column 'tweet': 22360 unique values\n",
      "  - Column 'sentiment': 3 unique values\n",
      "------------------------------\n",
      "\n",
      "--- DataFrame: 'df_generic' ---\n",
      "Counting unique values for each column:\n",
      "  - Column 'text_id': 27481 unique values\n",
      "  - Column 'text': 27476 unique values\n",
      "  - Column 'sentiment': 3 unique values\n",
      "------------------------------\n",
      "\n",
      "--- Fine Conteggio Globale ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Unique values count for every column of each DataFrame ---\")\n",
    "\n",
    "# Repeats on every couple name-DataFrame in the 'dataframes' dictionary\n",
    "for df_name, current_df in dataframes.items():\n",
    "    \n",
    "    print(f\"\\n--- DataFrame: '{df_name}' ---\")\n",
    "    print(f\"Counting unique values for each column:\")\n",
    "    \n",
    "    # Repeats on every column name present in the current DataFrame\n",
    "    for col_name in current_df.columns:\n",
    "        \n",
    "        # Elaborates the number of unique non-missing values for the current column\n",
    "        unique_count = current_df[col_name].dropna().nunique()\n",
    "        \n",
    "        # Prints the column name and the count\n",
    "        print(f\"  - Column '{col_name}': {unique_count} unique values\")\n",
    "        \n",
    "    print(\"-\" * 30) # Aggiunge una linea separatrice per leggibilità tra i DataFrame\n",
    "\n",
    "print(\"\\n--- Fine Conteggio Globale ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
